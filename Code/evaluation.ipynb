{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Translation\n",
    "TA_translation = \"/Users/maidang/Desktop/MSDS/Capstone Project/translation-agent/examples/csv/random_sample_TAtranslated.csv\"\n",
    "GL_translation = \"/Users/maidang/Desktop/MSDS/Capstone Project/translation-agent/examples/csv/random_sample_GLtranslated.csv\"\n",
    "groundtruth_set = \"/Users/maidang/Desktop/MSDS/Capstone Project/translation-agent/examples/csv/ground_true.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative BLEU score for the entire model: 0.2692784888609854\n",
      "BLEU scores saved to /Users/maidang/Desktop/MSDS/Capstone Project/translation-agent/examples/csv/TA_translation_with_bleu.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "\n",
    "output_csv = \"/Users/maidang/Desktop/MSDS/Capstone Project/translation-agent/examples/csv/TA_translation_with_bleu.csv\"\n",
    "# Load your agent's translations\n",
    "agent_translations = []\n",
    "with open(TA_translation, newline='') as agent_file:\n",
    "    reader = csv.reader(agent_file)\n",
    "    next(reader)  # Skip the header if there is one\n",
    "    for row in reader:\n",
    "        agent_translations.append((row[0], row[1]))  # (original_english, translated_vietnamese)\n",
    "\n",
    "# Load the ground truth translations\n",
    "ground_truth_translations = []\n",
    "with open(groundtruth_set, newline='') as gt_file:\n",
    "    reader = csv.reader(gt_file)\n",
    "    next(reader)  # Skip the header if there is one\n",
    "    for row in reader:\n",
    "        ground_truth_translations.append((row[0], row[2]))  # (Sentence, translation)\n",
    "\n",
    "# Prepare the data\n",
    "references = []  # List of lists of references (ground truth translations)\n",
    "candidates = []  # List of candidates (agent translations)\n",
    "bleu_scores = []  # List to store sentence-level BLEU scores\n",
    "\n",
    "# Calculate BLEU scores\n",
    "for translation, ground_truth in zip(agent_translations, ground_truth_translations):\n",
    "    if translation[0] == ground_truth[0]:  # Match sentences by English original\n",
    "        reference = [ground_truth[1].split()]  # Formal Vietnamese translation\n",
    "        candidate = translation[1].split()  # Agent Vietnamese translation\n",
    "        \n",
    "        # Calculate sentence-level BLEU score\n",
    "        bleu_score = sentence_bleu(reference, candidate)\n",
    "        bleu_scores.append((translation[0], translation[1], bleu_score))\n",
    "        \n",
    "        # Append to corpus-level lists\n",
    "        references.append(reference)  # Wrap reference in a list\n",
    "        candidates.append(candidate)  # Candidate translation\n",
    "    else:\n",
    "        print(f\"Mismatch found: '{translation[0]}' vs '{ground_truth[0]}'\")\n",
    "\n",
    "# Calculate the cumulative BLEU score for the entire corpus\n",
    "cumulative_bleu_score = corpus_bleu(references, candidates)\n",
    "\n",
    "# Print the cumulative BLEU score\n",
    "print(f\"Cumulative BLEU score for the entire model: {cumulative_bleu_score}\")\n",
    "\n",
    "# Save the sentence-level BLEU scores to a CSV file\n",
    "with open(output_csv, mode='w', newline='') as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerow(['Sentence', 'Translation', 'BLEU Score'])  # Header\n",
    "    writer.writerows(bleu_scores)\n",
    "\n",
    "print(f\"BLEU scores saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Translation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative BLEU score for the entire model: 0.3629226864964535\n",
      "BLEU scores saved to /Users/maidang/Desktop/MSDS/Capstone Project/translation-agent/examples/csv/GL_translation_with_bleu.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "\n",
    "output_csv = \"/Users/maidang/Desktop/MSDS/Capstone Project/translation-agent/examples/csv/GL_translation_with_bleu.csv\"\n",
    "# Load your agent's translations\n",
    "translations = []\n",
    "with open(GL_translation, newline='') as agent_file:\n",
    "    reader = csv.reader(agent_file)\n",
    "    next(reader)  # Skip the header if there is one\n",
    "    for row in reader:\n",
    "        translations.append((row[0], row[1]))  # (original_english, translated_vietnamese)\n",
    "\n",
    "# Load the ground truth translations\n",
    "ground_truth_translations = []\n",
    "with open(groundtruth_set, newline='') as gt_file:\n",
    "    reader = csv.reader(gt_file)\n",
    "    next(reader)  # Skip the header if there is one\n",
    "    for row in reader:\n",
    "        ground_truth_translations.append((row[0], row[2]))  # (Sentence, translation)\n",
    "\n",
    "# Prepare the data\n",
    "references = []  # List of lists of references (ground truth translations)\n",
    "candidates = []  # List of candidates (agent translations)\n",
    "bleu_scores = []  # List to store sentence-level BLEU scores\n",
    "\n",
    "# Calculate BLEU scores\n",
    "for translation, ground_truth in zip(translations, ground_truth_translations):\n",
    "    if translation[0] == ground_truth[0]:  # Match sentences by English original\n",
    "        reference = [ground_truth[1].split()]  # Formal Vietnamese translation\n",
    "        candidate = translation[1].split()  # Agent Vietnamese translation\n",
    "        \n",
    "        # Calculate sentence-level BLEU score\n",
    "        bleu_score = sentence_bleu(reference, candidate)\n",
    "        bleu_scores.append((translation[0], translation[1], bleu_score))\n",
    "        \n",
    "        # Append to corpus-level lists\n",
    "        references.append(reference)  # Wrap reference in a list\n",
    "        candidates.append(candidate)  # Candidate translation\n",
    "    else:\n",
    "        print(f\"Mismatch found: '{translation[0]}' vs '{ground_truth[0]}'\")\n",
    "\n",
    "# Calculate the cumulative BLEU score for the entire corpus\n",
    "cumulative_bleu_score = corpus_bleu(references, candidates)\n",
    "\n",
    "# Print the cumulative BLEU score\n",
    "print(f\"Cumulative BLEU score for the entire model: {cumulative_bleu_score}\")\n",
    "\n",
    "# Save the sentence-level BLEU scores to a CSV file\n",
    "with open(output_csv, mode='w', newline='') as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerow(['Sentence', 'Translation', 'BLEU Score'])  # Header\n",
    "    writer.writerows(bleu_scores)\n",
    "\n",
    "print(f\"BLEU scores saved to {output_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
